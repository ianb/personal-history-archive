{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get embeddings\n",
    "\n",
    "We have generated embeddings (number vectors) for both tags and class names. These are text files that look like `name num1 num2 ...`, one line per object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class example: ('-comment-item-option-tool', [0.267649, 0.060728, 0.372383, -0.167402, -0.42405, -0.213464, -0.18574, 0.22672, 0.159343, 0.14346, 0.225321, 0.350789, 0.218112, 0.276691, 0.447376, -0.192947, -0.290775, -0.259949, 0.388473, -0.310735, -0.424629, -0.472496, 0.188353, 0.150703, 0.112088, 0.265407, 0.222766, -0.08162, 0.402486, 0.190056, 0.430345, -0.476926, 0.11799, -0.422889, 0.111582, 0.288491, 0.013147, -0.005348, -0.018681, -0.291235, -0.016522, 0.251612, -0.322645, 0.318849, -0.48921, -0.307299, 0.004299, -0.581498, 0.13047, -0.641503])\n",
      "Tag example: ('nobr', [1.215299, 0.857834, 0.162706, 0.966696, 0.232542, 1.218955, 0.733365, 0.797194, -0.145894, -0.231142, -0.262402, -0.961403, -0.097864, -1.0834, 0.766679, 0.968583, 1.385928, 0.209797, -0.425753, -1.923545])\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "data = json.load(open(\"html-vectors.json\"))\n",
    "classname_vectors = data[\"classes\"]\n",
    "tag_vectors = data[\"tags\"]\n",
    "print(\"Class example:\", list(classname_vectors.items())[0])\n",
    "print(\"Tag example:\", list(tag_vectors.items())[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create vectors for elements\n",
    "\n",
    "Now, given an element, we need to make a vector. \n",
    "\n",
    "A tag has a few properties:\n",
    "* One tag name\n",
    "* Zero or more class names\n",
    "* A number of immediate children\n",
    "* A number of descendants (including grandchildren, etc)\n",
    "* A number of words\n",
    "* Number of links\n",
    "\n",
    "We'll turn these into several vectors, all of which are concatenated. Several use a parameter `min(n / range, 1.0)` which we'll call `trunc_count(n, range)`\n",
    "\n",
    "* The tag vector\n",
    "* The sum of the class name vectors (or the vector for `no-class`)\n",
    "* `trunc_count(children, 10)`\n",
    "* `trunc_count(desc, 100)`\n",
    "* `trunc_count(words, 250)`\n",
    "* `links / words` or 0 if there are no words\n",
    "\n",
    "FIXME: add `display` property (i.e., block, block-ish like inline-block, or not-block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pha import htmltools\n",
    "\n",
    "def trunc_count(n, max_count):\n",
    "    return min(n / max_count, 1.0)\n",
    "\n",
    "def element_to_vector(el):\n",
    "    children = len(el)\n",
    "    desc = len(list(el.iter()))\n",
    "    words = len(el.text_content().split())\n",
    "    links = len(el.cssselect('a'))\n",
    "    classnames = htmltools.normalize_classes(el)\n",
    "    classnames = [classname_vectors[c] for c in classnames if c in classname_vectors] or [classname_vectors['no-class']]\n",
    "    classname_sum = np.sum(classnames, axis=0)\n",
    "    tag_vector = tag_vectors.get(el.tag) or tag_vectors[\"div\"]\n",
    "    word_links = links / words if words else 0.0\n",
    "    vector = np.concatenate([\n",
    "        tag_vector,\n",
    "        classname_sum,\n",
    "        [trunc_count(children, 10), trunc_count(desc, 100), trunc_count(words, 250), word_links],\n",
    "    ])\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.342614e+00 -9.486100e-02  4.825000e-02 -2.420997e+00  1.879200e-01\n",
      "  1.059590e+00  2.564651e+00  4.462840e-01  6.541920e-01 -8.095740e-01\n",
      " -6.323590e-01 -3.787137e+00  1.598670e-01 -2.616020e-01  3.036870e-01\n",
      " -1.347139e+00  1.822867e+00 -1.205438e+00  9.460170e-01 -1.036174e+00\n",
      " -2.597010e-01 -3.733580e-01  1.566220e-01  1.588957e+00  1.118785e+00\n",
      " -1.518593e+00 -7.373980e-01  6.349290e-01  5.760940e-01  2.792740e-01\n",
      " -1.000190e-01 -3.212480e-01 -1.515810e-01  1.517445e+00 -4.100120e-01\n",
      "  4.545120e-01 -1.409812e+00  2.038540e-01 -7.124000e-02 -1.194100e-02\n",
      " -4.407500e-01 -7.227330e-01  1.052250e+00 -8.168990e-01  1.433370e-01\n",
      " -1.351080e-01  2.169070e-01 -8.799660e-01  2.570460e-01 -4.959670e-01\n",
      " -4.564300e-02 -6.928400e-02  1.093841e+00 -1.262750e-01  9.825410e-01\n",
      " -1.089900e-01 -3.273710e-01  1.234727e+00 -1.384890e-01  1.434000e-03\n",
      "  3.838530e-01 -1.019180e-01  8.018710e-01  7.520770e-01 -1.827950e-01\n",
      "  4.175690e-01 -5.354360e-01  7.710520e-01 -4.671780e-01 -2.141180e-01\n",
      "  0.000000e+00  1.000000e-02  1.600000e-02  0.000000e+00] (74,)\n"
     ]
    }
   ],
   "source": [
    "# Let's test it out:\n",
    "import lxml.html\n",
    "\n",
    "example_el = lxml.html.fromstring('<div class=\"login\">this is an element</div>')\n",
    "print(element_to_vector(example_el), element_to_vector(example_el).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training data\n",
    "\n",
    "The training (Y) data is fairly simple: is the element readable or not? We'll use a vector of `[is_not_readable, is_readable]` for each item (so always `[1, 0]` for non-readable elements, and `[0, 1]` for readable elements.\n",
    "\n",
    "But not all elements are either! Specifically elements *inside* a readable element are readable, but aren't what we are selecting. These elements will be filtered out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_testable_elements(doc):\n",
    "    body = list(doc.iter(\"body\"))[0]\n",
    "    def iter_children(el):\n",
    "        for child in el:\n",
    "            if child.get(\"data-isreadable\"):\n",
    "                yield True, child\n",
    "            else:\n",
    "                yield False, child\n",
    "                yield from iter_children(child)\n",
    "    return iter_children(body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(False, <Element div at 0x496f5eae8>), (True, <Element ul at 0x496f5e458>), (False, <Element ol at 0x496f5eea8>), (False, <Element li at 0x496f5e098>), (False, <Element li at 0x496f5eb88>)]\n"
     ]
    }
   ],
   "source": [
    "example_doc = lxml.html.document_fromstring(\"\"\"\n",
    "<html><head><title>Test</title></head>\n",
    "<body><div><ul data-isreadable=\"1\"><li>1<li>2</ul>\n",
    "<ol><li>3<li>4</ol>\n",
    "</div></body></html>\n",
    "\"\"\")\n",
    "print(list(iter_testable_elements(example_doc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual training data\n",
    "\n",
    "To get real training data we have to load the history, get all the elements, convert them to vectors, and construct results along the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Archive at '/Users/ianbicking/src/personal-history-archive' 19596/52900 fetched, 31046 errored>\n",
      "14995\n"
     ]
    }
   ],
   "source": [
    "import pha\n",
    "import pha.htmltools\n",
    "import random\n",
    "archive = pha.Archive.default_location()\n",
    "print(archive)\n",
    "histories = archive.histories_with_page()\n",
    "random.shuffle(histories)\n",
    "print(len(histories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 256890 with readable: 8640\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "Y = []\n",
    "total = 0\n",
    "with_readable = 0\n",
    "for h in histories:\n",
    "    for is_readable, el in iter_testable_elements(h.page.lxml):\n",
    "        if not is_readable and random.random() > 0.02:\n",
    "            continue\n",
    "        total += 1\n",
    "        X.append(element_to_vector(el))\n",
    "        if is_readable:\n",
    "            with_readable += 1\n",
    "        Y.append([0., 1.] if is_readable else [1., 0.])\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "print(\"Total:\", total, \"with readable:\", with_readable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "h5f = h5py.File('readable-training-data.h5', 'w')\n",
    "h5f.create_dataset('X_1', data=X, dtype=np.float32, compression=\"gzip\")\n",
    "h5f.create_dataset('Y_1', data=Y, dtype=np.int8, compression=\"gzip\")\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: <class 'numpy.ndarray'> <class 'numpy.ndarray'> 256890 (74,) sub-length: 74 size: 19 Y: 256890\n"
     ]
    }
   ],
   "source": [
    "print(\"X:\", type(X), type(X[0]), len(X), X[0].shape, \"sub-length:\", len(X[0]), \"size:\", len(X) * len(X[0]) // 1000000, \"Y:\", len(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14996243 14996243\n"
     ]
    }
   ],
   "source": [
    "h5f = h5py.File('readable-training-data.h5', 'r')\n",
    "print(len(h5f['X_1']), len(h5f['Y_1']))\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 8 * len(X) // 10\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "Y_train, Y_test = Y[:split], Y[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "def make_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(input_shape[0], input_shape=input_shape))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(256, input_shape=input_shape))\n",
    "    model.add(Dense(2, activation='softmax')) # is readable, or is not readable\n",
    "    model.compile(\n",
    "        optimizer='rmsprop', \n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = make_model(X[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.05133, saving model to readable-model-best.h5\n",
      "\n",
      "Epoch 00002: val_loss did not improve\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.05133 to 0.05114, saving model to readable-model-best.h5\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.05114 to 0.05059, saving model to readable-model-best.h5\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n",
      "\n",
      "Epoch 00021: val_loss did not improve\n",
      "\n",
      "Epoch 00022: val_loss did not improve\n",
      "\n",
      "Epoch 00023: val_loss did not improve\n",
      "\n",
      "Epoch 00024: val_loss did not improve\n",
      "\n",
      "Epoch 00025: val_loss did not improve\n",
      "\n",
      "Epoch 00026: val_loss did not improve\n",
      "\n",
      "Epoch 00027: val_loss did not improve\n",
      "\n",
      "Epoch 00028: val_loss did not improve\n",
      "\n",
      "Epoch 00029: val_loss did not improve\n",
      "\n",
      "Epoch 00030: val_loss did not improve\n",
      "\n",
      "Epoch 00031: val_loss did not improve\n",
      "\n",
      "Epoch 00032: val_loss did not improve\n",
      "\n",
      "Epoch 00033: val_loss did not improve\n",
      "\n",
      "Epoch 00034: val_loss did not improve\n",
      "\n",
      "Epoch 00035: val_loss did not improve\n",
      "\n",
      "Epoch 00036: val_loss did not improve\n",
      "\n",
      "Epoch 00037: val_loss did not improve\n",
      "\n",
      "Epoch 00038: val_loss did not improve\n",
      "\n",
      "Epoch 00039: val_loss did not improve\n",
      "\n",
      "Epoch 00040: val_loss did not improve\n",
      "\n",
      "Epoch 00041: val_loss did not improve\n",
      "\n",
      "Epoch 00042: val_loss did not improve\n",
      "\n",
      "Epoch 00043: val_loss did not improve\n",
      "\n",
      "Epoch 00044: val_loss did not improve\n",
      "\n",
      "Epoch 00045: val_loss did not improve\n",
      "\n",
      "Epoch 00046: val_loss did not improve\n",
      "\n",
      "Epoch 00047: val_loss did not improve\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.05059 to 0.05006, saving model to readable-model-best.h5\n",
      "\n",
      "Epoch 00049: val_loss did not improve\n",
      "\n",
      "Epoch 00050: val_loss did not improve\n",
      "Accuracy: [0.06527374820363409, 0.9771497528124878] ['loss', 'acc']\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpointer = ModelCheckpoint(\n",
    "    filepath='readable-model-best.h5', \n",
    "    verbose=1, save_best_only=True)\n",
    "hist = model.fit(\n",
    "    np.array(X_train), np.array(Y_train), \n",
    "    batch_size=128, epochs=50,\n",
    "    validation_split=0.2, \n",
    "    callbacks=[checkpointer],\n",
    "    verbose=0, shuffle=True)\n",
    "model.load_weights('readable-model-best.h5')\n",
    "score = model.evaluate(np.array(X_test), np.array(Y_test), verbose=0)\n",
    "print(\"Accuracy:\", score, model.metrics_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got readable right 826 and wrong 977\n",
      "Got unreadable right 49378 and wrong 197\n"
     ]
    }
   ],
   "source": [
    "right_readable = wrong_readable = right_unreadable = wrong_unreadable = 0\n",
    "for el, result in list(zip(X_test, Y_test)):\n",
    "    predict = model.predict(numpy.array([el]))\n",
    "    predict_readable = predict[0][0] < predict[0][1]\n",
    "    is_readable = result[0] < result[1]\n",
    "    if is_readable:\n",
    "        if predict_readable:\n",
    "            right_readable += 1\n",
    "        else:\n",
    "            wrong_readable += 1\n",
    "    elif predict_readable:\n",
    "        wrong_unreadable += 1\n",
    "    else:\n",
    "        right_unreadable += 1\n",
    "print(\"Got readable right\", right_readable, \"and wrong\", wrong_readable)\n",
    "print(\"Got unreadable right\", right_unreadable, \"and wrong\", wrong_unreadable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
